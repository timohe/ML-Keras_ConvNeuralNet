{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10_cnn_exam solution.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/timohe/ML-Keras_ConvNeuralNet/blob/master/cifar10_cnn_exam_solution.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "CwL1HaHTuWLt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CIFAR10 with CNN\n",
        "code from https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py\n"
      ]
    },
    {
      "metadata": {
        "id": "tkM8NCnJuWLv",
        "colab_type": "raw"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook trains a simple convolutional neural network on the CIFAR10 small images dataset. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "S8UZFSsBuWLv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from __future__ import print_function\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "#for confusion matrix\n",
        "!pip install -q pandas_ml\n",
        "import pandas_ml\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HX32-SrFuWL0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Prepare data"
      ]
    },
    {
      "metadata": {
        "id": "o07n3FvbuWL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0b4f95b9-d8e8-4c6d-ca02-51b5661c0d1b"
      },
      "cell_type": "code",
      "source": [
        "# define constants\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 2\n",
        "\n",
        "\n",
        "# The data, shuffled and split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Cast features into correct data type then scale features\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cW5LnHj5uWL3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define model"
      ]
    },
    {
      "metadata": {
        "id": "YYl_mjtUuWL4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MSNoYmMFuWL5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Show model structure"
      ]
    },
    {
      "metadata": {
        "id": "CF1UH3FsuWL6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "b9f428eb-6074-4d14-c959-5fb0efcbc00c"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,250,858\n",
            "Trainable params: 1,250,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5LrNtCcXuWL9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compile model and fit\n"
      ]
    },
    {
      "metadata": {
        "id": "j--3W_mEuWL9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "6e5f79d0-63f3-41f6-8c65-58de8ccdc428"
      },
      "cell_type": "code",
      "source": [
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_test, y_test),\n",
        "            shuffle=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "50000/50000 [==============================] - 258s 5ms/step - loss: 1.9720 - acc: 0.2783 - val_loss: 1.7261 - val_acc: 0.3880\n",
            "Epoch 2/2\n",
            "19456/50000 [==========>...................] - ETA: 2:30 - loss: 1.7402 - acc: 0.3698"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 258s 5ms/step - loss: 1.6847 - acc: 0.3860 - val_loss: 1.5494 - val_acc: 0.4430\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f74914a3f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "TnqygMfpuWMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "outputId": "f1b5fdd0-daa6-454d-cefe-cf9dfa07c140"
      },
      "cell_type": "code",
      "source": [
        "# Save the original model\n",
        "model.save('savedFiles/cifra10_base.h5')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1f28b10678be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'savedFiles/cifra10_base.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   2589\u001b[0m         \"\"\"\n\u001b[1;32m   2590\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2591\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = 'savedFiles/cifra10_base.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "qjAnVrbjuWMH",
        "colab_type": "code",
        "colab": {},
        "outputId": "2a5cb27d-7411-456c-bb63-743db136b0cf"
      },
      "cell_type": "code",
      "source": [
        "# Print base model loss and accuracy\n",
        "print('\\n  - Base case:')\n",
        "model = keras.models.load_model(\"savedFiles/cifra10_base.h5\")\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "y_test_predict = model.predict(self, x_test)\n",
        "print(pandas_ml.ConfusionMatrix(y_test, y_test_predict))\n",
        "print('\\tTest loss:\\t', scores[0])\n",
        "print('\\tTest accuracy:\\t', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  - Base case:\n",
            "\tTest loss:\t 0.8829086751937866\n",
            "\tTest accuracy:\t 0.69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AMBbUhcTuWMJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1) Augment the data by adding noise. Discuss results.\n",
        "\n",
        "# Save models with gaussian noise with different Standard Deviation\n",
        "\n",
        "for e in [0.08,0.07,0.06,0.05,0.04,0.03,0.02,0.01]:\n",
        "    model = keras.models.load_model(\"savedFiles/cifra10_base.h5\")\n",
        "    model.add(keras.layers.GaussianNoise(e))\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=opt,\n",
        "                    metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=epochs,\n",
        "                validation_data=(x_test, y_test),\n",
        "                shuffle=True)\n",
        "    model.save('savedFiles/cifra10_n'+str(e)+'.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fVUkk-43uWML",
        "colab_type": "code",
        "colab": {},
        "outputId": "16ffc791-4cdb-4807-f146-fda87da35735"
      },
      "cell_type": "code",
      "source": [
        "# Load models and print results\n",
        "for e in [0.08,0.07,0.06,0.05,0.04,0.03,0.02,0.01]:\n",
        "    model = keras.models.load_model('savedFiles/cifra10_n'+str(e)+'.h5')\n",
        "    print('\\n  - Gaussian Noise with '+str(e)+' stdev:')\n",
        "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print('\\tTest loss:\\t', scores[0])\n",
        "    print('\\tTest accuracy:\\t', scores[1])\n",
        "    \n",
        "# Load and print of the best loss and accuracy obtained\n",
        "model = keras.models.load_model(\"savedFiles/cifra10_n0.02.h5\")\n",
        "print('\\n\\nBest results:')\n",
        "print('  - Gaussian Noise with 0.02 stdev:')\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('\\tTest loss:\\t', scores[0])\n",
        "print('\\tTest accuracy:\\t', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  - Gaussian Noise with 0.08 stdev:\n",
            "\tTest loss:\t 0.830376790714264\n",
            "\tTest accuracy:\t 0.7118\n",
            "\n",
            "  - Gaussian Noise with 0.07 stdev:\n",
            "\tTest loss:\t 0.8354920146942139\n",
            "\tTest accuracy:\t 0.7119\n",
            "\n",
            "  - Gaussian Noise with 0.06 stdev:\n",
            "\tTest loss:\t 0.8408714697837829\n",
            "\tTest accuracy:\t 0.7097\n",
            "\n",
            "  - Gaussian Noise with 0.05 stdev:\n",
            "\tTest loss:\t 0.7833409372329712\n",
            "\tTest accuracy:\t 0.7291\n",
            "\n",
            "  - Gaussian Noise with 0.04 stdev:\n",
            "\tTest loss:\t 0.803309340763092\n",
            "\tTest accuracy:\t 0.7226\n",
            "\n",
            "  - Gaussian Noise with 0.03 stdev:\n",
            "\tTest loss:\t 0.782099357509613\n",
            "\tTest accuracy:\t 0.735\n",
            "\n",
            "  - Gaussian Noise with 0.02 stdev:\n",
            "\tTest loss:\t 0.7369885484695434\n",
            "\tTest accuracy:\t 0.742\n",
            "\n",
            "  - Gaussian Noise with 0.01 stdev:\n",
            "\tTest loss:\t 0.830383843421936\n",
            "\tTest accuracy:\t 0.7174\n",
            "\n",
            "\n",
            "Best results:\n",
            "  - Gaussian Noise with 0.02 stdev:\n",
            "\tTest loss:\t 0.7369885484695434\n",
            "\tTest accuracy:\t 0.742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f2KL0hWjuWMO",
        "colab_type": "code",
        "colab": {},
        "outputId": "aa6b71ff-683b-4a8b-cd83-f24267e65083"
      },
      "cell_type": "code",
      "source": [
        "# 2) Add layers to the network to see if can improve performance \n",
        "# - try two additional configurations - provide short discussin of what you did and why. \n",
        "# In addition, discuss results and compare and contrast confusion matrices from the three \n",
        "# different configurations (original and your two creations).\n",
        "\n",
        "# First model: batch normalization\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "                metrics=['accuracy'])\n",
        "model.fit(x_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_test, y_test),\n",
        "            shuffle=True)\n",
        "\n",
        "model.save('savedFiles/cifra10_batch.h5')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 296s 6ms/step - loss: 1.9490 - acc: 0.3041 - val_loss: 1.5028 - val_acc: 0.4599\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 302s 6ms/step - loss: 1.5574 - acc: 0.4348 - val_loss: 1.3246 - val_acc: 0.5235\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 302s 6ms/step - loss: 1.4228 - acc: 0.4863 - val_loss: 1.2613 - val_acc: 0.5494\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 302s 6ms/step - loss: 1.3397 - acc: 0.5174 - val_loss: 1.1878 - val_acc: 0.5791\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 302s 6ms/step - loss: 1.2701 - acc: 0.5476 - val_loss: 1.1172 - val_acc: 0.6039\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 302s 6ms/step - loss: 1.2189 - acc: 0.5671 - val_loss: 1.0932 - val_acc: 0.6082\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 301s 6ms/step - loss: 1.1627 - acc: 0.5845 - val_loss: 1.0218 - val_acc: 0.6347\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 274s 5ms/step - loss: 1.1163 - acc: 0.6041 - val_loss: 1.0478 - val_acc: 0.6261\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 291s 6ms/step - loss: 1.0765 - acc: 0.6162 - val_loss: 1.0049 - val_acc: 0.6392\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 283s 6ms/step - loss: 1.0399 - acc: 0.6294 - val_loss: 0.9357 - val_acc: 0.6670\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 260s 5ms/step - loss: 1.0010 - acc: 0.6435 - val_loss: 0.8964 - val_acc: 0.6870\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 275s 6ms/step - loss: 0.9731 - acc: 0.6548 - val_loss: 0.9009 - val_acc: 0.6850\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 327s 7ms/step - loss: 0.9458 - acc: 0.6660 - val_loss: 0.8412 - val_acc: 0.7018\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 577s 12ms/step - loss: 0.9206 - acc: 0.6740 - val_loss: 0.8401 - val_acc: 0.7011\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 332s 7ms/step - loss: 0.8910 - acc: 0.6851 - val_loss: 0.8681 - val_acc: 0.6954\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 543s 11ms/step - loss: 0.8733 - acc: 0.6941 - val_loss: 0.8873 - val_acc: 0.6914\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 645s 13ms/step - loss: 0.8521 - acc: 0.6971 - val_loss: 0.8104 - val_acc: 0.7176\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 330s 7ms/step - loss: 0.8279 - acc: 0.7053 - val_loss: 0.8370 - val_acc: 0.7105\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 274s 5ms/step - loss: 0.8140 - acc: 0.7120 - val_loss: 0.7535 - val_acc: 0.7367\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 256s 5ms/step - loss: 0.7969 - acc: 0.7180 - val_loss: 0.7872 - val_acc: 0.7238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L-1gJhvUuWMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "c089cfa8-2cea-4965-99dd-cf8be1f52887"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model = keras.models.load_model('savedFiles/cifra10_batch.h5')\n",
        "print('\\n  - Model with Batch Normalization:')\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('\\tTest loss:\\t', scores[0])\n",
        "print('\\tTest accuracy:\\t', scores[1])\n",
        "\n",
        "\n",
        "y_pred_test = model.predict(x_test, y_test, verbose=0)\n",
        "print(pandas_ml.ConfusionMatrix(y_test, y_pred_test))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e7d84b6eb988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'savedFiles/cifra10_batch.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n  - Model with Batch Normalization:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\tTest loss:\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "WJ4IGqDfuWME",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Exercises:\n",
        "1. Augment the data by adding noise.  Discuss results.\n",
        "2. Add layers to the network to see if can improve performance - try two additional configurations - provide short discussin of what you did and why.  In addition, discuss results and compare and contrast confusion matrices from the three different configurations (original and your two creations). \n",
        "3. Try another method for dealing with overfitting. Discuss results."
      ]
    },
    {
      "metadata": {
        "id": "ptxx_FTouWMU",
        "colab_type": "code",
        "colab": {},
        "outputId": "1cf79319-6518-49f5-fc53-de4dd44b53c1"
      },
      "cell_type": "code",
      "source": [
        "# 2) Add layers to the network to see if can improve performance \n",
        "# - try two additional configurations - provide short discussin of what you did and why. \n",
        "# In addition, discuss results and compare and contrast confusion matrices from the three \n",
        "# different configurations (original and your two creations).\n",
        "\n",
        "# Second model: regularizer\n",
        "\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras import regularizers\n",
        "\n",
        "weight_decay = 1e-4\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "                metrics=['accuracy'])\n",
        "model.fit(x_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_test, y_test),\n",
        "            shuffle=True)\n",
        "\n",
        "model.save('savedFiles/cifra10_regL2.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 139s 3ms/step - loss: 2.0065 - acc: 0.2682 - val_loss: 1.7413 - val_acc: 0.3876\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 137s 3ms/step - loss: 1.7118 - acc: 0.3826 - val_loss: 1.5890 - val_acc: 0.4288\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 139s 3ms/step - loss: 1.5883 - acc: 0.4279 - val_loss: 1.4806 - val_acc: 0.4748\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 137s 3ms/step - loss: 1.4952 - acc: 0.4624 - val_loss: 1.4071 - val_acc: 0.5048\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 136s 3ms/step - loss: 1.4269 - acc: 0.4901 - val_loss: 1.3790 - val_acc: 0.5182\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 139s 3ms/step - loss: 1.3737 - acc: 0.5120 - val_loss: 1.2660 - val_acc: 0.5527\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 136s 3ms/step - loss: 1.3281 - acc: 0.5305 - val_loss: 1.2483 - val_acc: 0.5624\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 137s 3ms/step - loss: 1.2835 - acc: 0.5479 - val_loss: 1.2020 - val_acc: 0.5788\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 138s 3ms/step - loss: 1.2500 - acc: 0.5618 - val_loss: 1.2061 - val_acc: 0.5842\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 136s 3ms/step - loss: 1.2157 - acc: 0.5750 - val_loss: 1.1331 - val_acc: 0.6048\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 137s 3ms/step - loss: 1.1789 - acc: 0.5885 - val_loss: 1.1163 - val_acc: 0.6079\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 137s 3ms/step - loss: 1.1534 - acc: 0.5956 - val_loss: 1.0698 - val_acc: 0.6245\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 136s 3ms/step - loss: 1.1226 - acc: 0.6076 - val_loss: 1.0745 - val_acc: 0.6245\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 137s 3ms/step - loss: 1.0987 - acc: 0.6166 - val_loss: 1.0150 - val_acc: 0.6494\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 136s 3ms/step - loss: 1.0702 - acc: 0.6276 - val_loss: 0.9963 - val_acc: 0.6575\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 136s 3ms/step - loss: 1.0504 - acc: 0.6349 - val_loss: 0.9977 - val_acc: 0.6570\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 136s 3ms/step - loss: 1.0270 - acc: 0.6447 - val_loss: 0.9958 - val_acc: 0.6587\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 136s 3ms/step - loss: 1.0066 - acc: 0.6514 - val_loss: 0.9707 - val_acc: 0.6685\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 136s 3ms/step - loss: 0.9879 - acc: 0.6585 - val_loss: 0.9335 - val_acc: 0.6780\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 137s 3ms/step - loss: 0.9685 - acc: 0.6655 - val_loss: 0.9597 - val_acc: 0.6666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kdaY9vZpuWMa",
        "colab_type": "code",
        "colab": {},
        "outputId": "c7f35df9-13b8-435a-e0cd-10bff7cca467"
      },
      "cell_type": "code",
      "source": [
        "model= keras.models.load_model('savedFiles/cifra10_regL2.h5')\n",
        "print('\\n  - regularizer L2:')\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('\\tTest loss:\\t', scores[0])\n",
        "print('\\tTest accuracy:\\t', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  - regularizer L2:\n",
            "\tTest loss:\t 0.9596688842773438\n",
            "\tTest accuracy:\t 0.6666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S7i58d-1uWMc",
        "colab_type": "code",
        "colab": {},
        "outputId": "a1508d27-3c63-4708-e9ac-3cbebe0d5016"
      },
      "cell_type": "code",
      "source": [
        "# 3) Try another method for dealing with overfitting. Discuss results.\n",
        "\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras import regularizers\n",
        "\n",
        "weight_decay = 1e-4\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "                metrics=['accuracy'])\n",
        "model.fit(x_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_test, y_test),\n",
        "            shuffle=True)\n",
        "\n",
        "model.save('savedFiles/cifra10_batch_regL2.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 264s 5ms/step - loss: 1.9654 - acc: 0.3062 - val_loss: 1.5571 - val_acc: 0.4412\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 255s 5ms/step - loss: 1.5817 - acc: 0.4248 - val_loss: 1.3868 - val_acc: 0.5038\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 256s 5ms/step - loss: 1.4555 - acc: 0.4754 - val_loss: 1.3421 - val_acc: 0.5254\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 255s 5ms/step - loss: 1.3685 - acc: 0.5142 - val_loss: 1.2107 - val_acc: 0.5723\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 256s 5ms/step - loss: 1.3007 - acc: 0.5375 - val_loss: 1.2377 - val_acc: 0.5710\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 257s 5ms/step - loss: 1.2406 - acc: 0.5586 - val_loss: 1.1629 - val_acc: 0.5944\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 258s 5ms/step - loss: 1.1893 - acc: 0.5786 - val_loss: 1.0853 - val_acc: 0.6181\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 287s 6ms/step - loss: 1.1376 - acc: 0.5983 - val_loss: 1.0449 - val_acc: 0.6348\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 256s 5ms/step - loss: 1.0965 - acc: 0.6139 - val_loss: 1.0597 - val_acc: 0.6273\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 255s 5ms/step - loss: 1.0587 - acc: 0.6269 - val_loss: 1.0164 - val_acc: 0.6463\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 254s 5ms/step - loss: 1.0284 - acc: 0.6403 - val_loss: 0.9629 - val_acc: 0.6592\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 255s 5ms/step - loss: 0.9882 - acc: 0.6530 - val_loss: 1.0084 - val_acc: 0.6483\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 257s 5ms/step - loss: 0.9700 - acc: 0.6611 - val_loss: 0.9794 - val_acc: 0.6605\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 300s 6ms/step - loss: 0.9367 - acc: 0.6719 - val_loss: 0.9012 - val_acc: 0.6852\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 270s 5ms/step - loss: 0.9067 - acc: 0.6794 - val_loss: 0.9267 - val_acc: 0.6819\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 257s 5ms/step - loss: 0.8833 - acc: 0.6910 - val_loss: 0.8380 - val_acc: 0.7085\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 263s 5ms/step - loss: 0.8626 - acc: 0.6973 - val_loss: 0.8669 - val_acc: 0.7009\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 260s 5ms/step - loss: 0.8472 - acc: 0.7058 - val_loss: 0.8254 - val_acc: 0.7126\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 280s 6ms/step - loss: 0.8269 - acc: 0.7123 - val_loss: 0.8339 - val_acc: 0.7127\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 260s 5ms/step - loss: 0.8107 - acc: 0.7169 - val_loss: 0.7825 - val_acc: 0.7310\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jpd39SHkuWMf",
        "colab_type": "code",
        "colab": {},
        "outputId": "c959bbd6-401b-47e0-e29d-7f4b71f7952a"
      },
      "cell_type": "code",
      "source": [
        "model.save('savedFiles/cifra10_batch_regL2.h5')\n",
        "print('\\n  - Model with batch normalization and regularizer:')\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('\\tTest loss:\\t', scores[0])\n",
        "print('\\tTest accuracy:\\t', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  - Model with batch normalization and regularizer:\n",
            "\tTest loss:\t 0.782516943359375\n",
            "\tTest accuracy:\t 0.731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mxwsz6ZUuWMh",
        "colab_type": "code",
        "colab": {},
        "outputId": "98431b86-99dc-4f95-d231-9e832d28d69f"
      },
      "cell_type": "code",
      "source": [
        "# 3) Try another method for dealing with overfitting. Discuss results.\n",
        "\n",
        "model = keras.models.load_model('savedFiles/cifra10_batch.h5')\n",
        "model.add(keras.layers.GaussianNoise(0.01))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "                metrics=['accuracy'])\n",
        "model.fit(x_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_test, y_test),\n",
        "            shuffle=True)\n",
        "model.save('savedFiles/cifra10_batch_n0.01.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 264s 5ms/step - loss: 0.8254 - acc: 0.7253 - val_loss: 0.7493 - val_acc: 0.7401\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 257s 5ms/step - loss: 0.8272 - acc: 0.7266 - val_loss: 0.7915 - val_acc: 0.7283\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 256s 5ms/step - loss: 0.8099 - acc: 0.7335 - val_loss: 0.7576 - val_acc: 0.7412\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 256s 5ms/step - loss: 0.7952 - acc: 0.7338 - val_loss: 0.7867 - val_acc: 0.7304\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 257s 5ms/step - loss: 0.7850 - acc: 0.7397 - val_loss: 0.7258 - val_acc: 0.7470\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 260s 5ms/step - loss: 0.7818 - acc: 0.7436 - val_loss: 0.7408 - val_acc: 0.7446\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 296s 6ms/step - loss: 0.7811 - acc: 0.7459 - val_loss: 0.7435 - val_acc: 0.7465\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 538s 11ms/step - loss: 0.7527 - acc: 0.7493 - val_loss: 0.7076 - val_acc: 0.7577\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 616s 12ms/step - loss: 0.7484 - acc: 0.7517 - val_loss: 0.6961 - val_acc: 0.7596\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 359s 7ms/step - loss: 0.7413 - acc: 0.7561 - val_loss: 0.7099 - val_acc: 0.7549\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 288s 6ms/step - loss: 0.7323 - acc: 0.7616 - val_loss: 0.7103 - val_acc: 0.7537\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 330s 7ms/step - loss: 0.7201 - acc: 0.7638 - val_loss: 0.7059 - val_acc: 0.7567\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 295s 6ms/step - loss: 0.7067 - acc: 0.7646 - val_loss: 0.6688 - val_acc: 0.7662\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 299s 6ms/step - loss: 0.7000 - acc: 0.7672 - val_loss: 0.6773 - val_acc: 0.7628\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 257s 5ms/step - loss: 0.6874 - acc: 0.7720 - val_loss: 0.6717 - val_acc: 0.7670\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 256s 5ms/step - loss: 0.6915 - acc: 0.7729 - val_loss: 0.6907 - val_acc: 0.7632\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 261s 5ms/step - loss: 0.6834 - acc: 0.7764 - val_loss: 0.7132 - val_acc: 0.7553\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 285s 6ms/step - loss: 0.6703 - acc: 0.7768 - val_loss: 0.6970 - val_acc: 0.7642\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 298s 6ms/step - loss: 0.6672 - acc: 0.7800 - val_loss: 0.6464 - val_acc: 0.7736\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 305s 6ms/step - loss: 0.6576 - acc: 0.7799 - val_loss: 0.6424 - val_acc: 0.7760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gmCzNoVQuWMl",
        "colab_type": "code",
        "colab": {},
        "outputId": "33348eab-6587-4ab2-cd6a-398686db25ba"
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model('savedFiles/cifra10_batch_n0.01.h5')\n",
        "\n",
        "print('\\n  - Model with Batch Normalization and Gaussian Noise with 0.01 stdev:')\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('\\tTest loss:\\t', scores[0])\n",
        "print('\\tTest accuracy:\\t', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  - Model with Batch Normalization and Gaussian Noise with 0.01 stdev:\n",
            "\tTest loss:\t 0.6423829383373261\n",
            "\tTest accuracy:\t 0.776\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}